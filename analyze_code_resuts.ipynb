{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5'\n",
    "benchmark = 'HumanEval' #'APPS-codewars'\n",
    "\n",
    "def get_results(task_id):\n",
    "    iter = 0\n",
    "    acc_dict = {}\n",
    "    for strategy in ['baseline', 'active-reasoning', 'baseline-binary', 'active-reasoning-binary']:\n",
    "        acc_dict[strategy] = {}\n",
    "        for iter in range(5):\n",
    "            acc_dict[strategy][iter] = []\n",
    "            for run_seed in [0, 1, 2]:\n",
    "                try:\n",
    "                    results_dict = json.load(open(f'./results/code-generation/{benchmark}/{task_id}/{strategy}/{model}/iter_{run_seed}/eval_program_correctness.json', 'r'))\n",
    "                except(FileNotFoundError):\n",
    "                    continue\n",
    "                for eval_seed in range(3):\n",
    "                    try:\n",
    "                        r_ls = results_dict[str(eval_seed)][str(iter)]\n",
    "                        try:\n",
    "                            acc = len([r for r in r_ls if r == 'True']) / len(r_ls)\n",
    "                        except(ZeroDivisionError):\n",
    "                            acc = None\n",
    "                    except(KeyError):\n",
    "                        acc = None\n",
    "                    acc_dict[strategy][iter].append(acc)\n",
    "    return acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>task_id</th>\n",
       "      <th>iter</th>\n",
       "      <th>seed</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   strategy task_id  iter  seed   acc\n",
       "0  baseline       5     0     0  25.0\n",
       "1  baseline       5     0     1  10.0\n",
       "2  baseline       5     0     2  10.0\n",
       "3  baseline       5     0     3  25.0\n",
       "4  baseline       5     0     4  10.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df = pd.DataFrame()\n",
    "\n",
    "id_ls = [\n",
    "    1, 5, 6, 17, 26, 33, 36, \n",
    "    39, 41, 54, 55, 64, 70, 81, 96, 106, 109, \n",
    "    147, 93, 118, 101, 143, 121, 134, 139, 141,\n",
    "    122, 82, 115, 77, 98, 90, 138, 74, 95, 110, 123,\n",
    "    111, 154, 114, 91, 103, 107, 76, 159, 73\n",
    "] # ids for HumanEval\n",
    "\n",
    "# id_ls = [1614, 2664, 2671, 2681, 2717, 2728, 2881, 2927, 2939, 2991, 3016,\n",
    "#        3068, 3072, 3079, 3220, 3248, 3278, 3366, 3443, 3452, 3477, 3536,\n",
    "#        3589, 3594, 3598, 3689, 3706, 3715, 3786, 3822, 3856, 3958, 4084,\n",
    "#        4128, 4182, 4190, 4240, 4293, 4315, 4317, 4326, 4353, 4360, 4414,\n",
    "#        4438, 4513, 4546] # ids for APPS\n",
    "\n",
    "for i in id_ls:\n",
    "    task_id = f'{i}'\n",
    "    acc_dict = get_results(task_id)\n",
    "    for key, results in acc_dict.items():\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        results_df.index = pd.MultiIndex.from_tuples(zip([key] * 5,  [task_id] * 5, range(5)))\n",
    "        all_results_df = pd.concat([all_results_df, results_df])\n",
    "\n",
    "all_results_df = all_results_df.stack().reset_index()\n",
    "all_results_df.columns = ['strategy', 'task_id', 'iter', 'seed', 'acc']\n",
    "all_results_df['acc'] = all_results_df['acc'] * 100\n",
    "\n",
    "all_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "strategy & baseline-binary & active-reasoning-binary & baseline & active-reasoning \\\\\n",
      "iter &  &  &  &  \\\\\n",
      "\\midrule\n",
      "0 & 44.1 \\scriptsize{(1.9)} & 44.6 \\scriptsize{(1.8)} & 47.1 \\scriptsize{(1.4)} & 47.0 \\scriptsize{(1.4)} \\\\\n",
      "1 & 55.3 \\scriptsize{(2.5)} & 66.8 \\scriptsize{(2.5)} & 67.5 \\scriptsize{(1.7)} & 74.4 \\scriptsize{(1.6)} \\\\\n",
      "2 & 65.2 \\scriptsize{(2.5)} & 78.4 \\scriptsize{(2.1)} & 71.6 \\scriptsize{(1.7)} & 81.4 \\scriptsize{(1.6)} \\\\\n",
      "3 & 70.7 \\scriptsize{(2.6)} & 82.2 \\scriptsize{(2.1)} & 75.5 \\scriptsize{(1.7)} & 84.5 \\scriptsize{(1.5)} \\\\\n",
      "4 & 70.8 \\scriptsize{(2.6)} & 85.6 \\scriptsize{(2.0)} & 75.9 \\scriptsize{(1.7)} & 85.0 \\scriptsize{(1.5)} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def std_err(x):\n",
    "    return np.std(x) / np.sqrt(len(x))\n",
    "\n",
    "print(\n",
    "(all_results_df.pivot_table(index='iter', columns='strategy', values='acc').map(lambda x: np.round(x, 1)).astype(str)\n",
    "+ ' \\scriptsize{(' +\n",
    "all_results_df.pivot_table(index='iter', columns='strategy', values='acc', aggfunc=std_err).map(lambda x: np.round(x, 1)).astype(str)\n",
    "+ ')}'\n",
    ")[['baseline-binary', 'active-reasoning-binary', 'baseline', 'active-reasoning']].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10912/172961200.py:1: FutureWarning: The provided callable <function mean at 0x7fcf28628180> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  llm_df = all_results_df.groupby(['strategy', 'iter'])['acc'].agg([np.mean, std_err]).reset_index()\n"
     ]
    }
   ],
   "source": [
    "llm_df = all_results_df.groupby(['strategy', 'iter'])['acc'].agg([np.mean, std_err]).reset_index()\n",
    "llm_df.rename({'std_err' : 'std'}, inplace=True, axis=1)\n",
    "llm_df['model'] = model\n",
    "llm_df['method'] = llm_df['strategy'].map({\n",
    "    'active-reasoning': 'EIG (O)',\n",
    "    'baseline': 'base (O)',\n",
    "    'active-reasoning-binary': 'EIG (B)',\n",
    "    'baseline-binary': 'base (B)'\n",
    "})\n",
    "llm_df[['model', 'method', 'iter', 'mean', 'std']].to_csv(f'./results/code-generation/{benchmark}/{model}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
